#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Mar  7 13:55:28 2018

@author: carsonluuu
"""

import json
import pickle
import random

import numpy as np
import matplotlib.pyplot as plt

from nltk.tokenize import RegexpTokenizer #remove pun
from nltk.stem.wordnet import WordNetLemmatizer

from sklearn.decomposition import TruncatedSVD
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction import text #stop words
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

from sklearn import metrics
from sklearn.metrics import roc_curve, auc
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression

def loadfile():
    file = open('tweet_data/tweets_#superbowl.txt')

    tweet = []
    y     = []
    for l in file:
        data = json.loads(l)
        if washington(data['tweet']['user']['location']):
            tweet.append(data['highlight'])
            y.append(0)
        elif newEngland(data['tweet']['user']['location']):
            tweet.append(data['highlight'])
            y.append(1)
    
    tweet_processed = textRegex(tweet)
    return tweet_processed, y

def washington(location):
    Washington = pickle.load( open( "Washington.p", "rb" ) )
    for area in ['DC', 'D.C.', 'dc', 'd.c.']:
        if area in location:
            return False
    for area in Washington:
        if area in location:
            return True
    return False

def newEngland(location):
    NewEngland = pickle.load( open( "NewEngland.p", "rb" ) )
    for area in NewEngland:
        if area in location:
            return True
    return False        

def textPreprocess(trainData):
    #import nltk
    #nltk.download('wordnet')     
    tokenizer = RegexpTokenizer(r'\w+')
    lmtzr = WordNetLemmatizer()
    #Lemmatizer eg. going -> go(v); nicer -> nice(a); cars -> car(n) 
    trainData = [tokenizer.tokenize(sentence) for sentence in trainData]
    trainData = [" ".join(sentence) for sentence in trainData]
    trainData = [[lmtzr.lemmatize(word, 'n') for word in sentence.split(" ")] for sentence in trainData]
    trainData = [" ".join(sentence) for sentence in trainData]
    trainData = [[lmtzr.lemmatize(word, 'v') for word in sentence.split(" ")] for sentence in trainData] 
    trainData = [" ".join(sentence) for sentence in trainData]
    trainData = [[lmtzr.lemmatize(word, 'a') for word in sentence.split(" ")] for sentence in trainData] 
    trainData = [" ".join(sentence) for sentence in trainData]        
    return trainData

def textRegex(data):
    tokenizer = RegexpTokenizer(r'\w+')
    data = [tokenizer.tokenize(sentence) for sentence in data]
    data = [" ".join(sentence) for sentence in data] 
    return data

def TFxIDF(trainData):
    # return documnt frequency
    stop_words = text.ENGLISH_STOP_WORDS
    vectorizer = CountVectorizer(stop_words=stop_words, lowercase=True, min_df=3, max_df=0.9)
    dataTrainCounts = vectorizer.fit_transform(trainData)
    transformer = TfidfTransformer()
    tfidf = transformer.fit_transform(dataTrainCounts)
    return tfidf
 
def lsa(data):    
    svd = TruncatedSVD(n_components=100, n_iter=10,random_state=42)
    train_data_lsa = svd.fit_transform(data)
#    res = svd.singular_values_
#    res = svd.explained_variance_ratio_
    return train_data_lsa

def performance(y_true, y_label):
    """
    Calculates the performance metric based on the agreement between the 
    true labels and the predicted labels.
    
    """
    print(metrics.accuracy_score(y_true, y_label))
    print(metrics.f1_score(y_true, y_label))
    print(metrics.precision_score(y_true, y_label))
    print(metrics.recall_score(y_true, y_label))
    print(metrics.confusion_matrix(y_true, y_label))


def cv_performance(clf, X, y, kf, metric="accuracy"):
    """
    Splits the data, X and y, into k-folds and runs k-fold cross-validation.
    Trains classifier on k-1 folds and tests on the remaining fold.
    
    """
    scores = []
    for train_index, test_index in kf :
#        print("TRAIN:", train_index, "TEST:", test_index)
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        clf.fit(X_train, y_train)
        y_pred = clf.decision_function(X_test)
        scores.append(performance(y_test, y_pred, metric = metric))
    return np.mean(scores)

def roc(test_label, predicted_prob):
    """
    roc curve and area    
    """
    fpr, tpr, thresholds = roc_curve(test_label, predicted_prob)
    auroc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label='ROC curve (auroc = %0.3f)' % auroc)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve--Hard Margin SVC using LSI')
    plt.legend(loc="lower right")
    plt.show()

def pipeline(model_clf):
    clf = Pipeline([('vect', CountVectorizer(stop_words=text.ENGLISH_STOP_WORDS, lowercase=True, min_df=2, max_df=0.99)),
                    ('tfidf', TfidfTransformer()),
                    ('svd', TruncatedSVD(n_components=100, n_iter=10, random_state=42)),
                    ('clf', model_clf)])
    return clf

def svc():
    return SVC(kernel='linear', probability=True)

def LR():
    return LogisticRegression()

def RFC():
    return RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)

def adaBoost():
    return AdaBoostClassifier()

def NNC():
    return MLPClassifier(alpha=1)

if __name__ == "__main__":
    
#    pickle.dump(tweet_processed, open("tweet_processed.p", "wb") )
#    pickle.dump(y, open("tweet_processed_label.p", "wb") )
    y = pickle.load( open("tweet_processed_label.p", "rb") )
    tweet_processed = pickle.load( open("tweet_processed.p", "rb") )

#    tweet_tfidf = TFxIDF(tweet_processed)
#    print(tweet_tfidf.shape)
#    hard_svc_clf = Pipeline([('vect', CountVectorizer(stop_words=text.ENGLISH_STOP_WORDS, lowercase=True, min_df=2, max_df=0.99)),
#                         ('tfidf', TfidfTransformer()),
#                         ('svd', TruncatedSVD(n_components=100, n_iter=10, random_state=42)),
#                         ('clf', SVC(kernel='linear', probability=True)),
#    ])
    
    X = np.array(tweet_processed)
    y = np.array(y)
    
    train_feature = X[:5000]
    train_label   = y[:5000]
    test_feature  = X[:5000]
    test_label    = y[:5000]
#    test_feature  = X[53000:]
#    test_label    = y[53000:]
    
#    train_feature = lsa(TFxIDF(train_feature))    
#    hard_svc_clf = SVC(kernel='linear', probability=True)
    
    pipeline(svc).fit(train_feature, train_label)
    
    predicted_prob = pipeline.predict_proba(test_feature)[:, 1]
    predicted = pipeline.predict(test_feature)
    
    
#    print(metrics.confusion_matrix(test_label, predicted))
    performance(test_label, predicted)
    roc(test_label, predicted_prob)

    
